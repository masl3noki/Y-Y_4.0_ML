{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "119c9460",
   "metadata": {},
   "source": [
    "## Home assignment 06: Gradient boosting with MSE\n",
    "\n",
    "Please, fill the lines in the code below.\n",
    "This is a simplified version of `BoostingRegressor` from `sklearn`. Please, notice, that `sklearn` API is **not preserved**.\n",
    "\n",
    "Your algorithm should be able to train different numbers of instances of the same model class. Every target is computed according to the loss function gradient. In this particular case, loss is computed for MSE.\n",
    "\n",
    "The model should be passed as model class with no explicit parameters and no parentheses.\n",
    "\n",
    "Example:\n",
    "```\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "boosting_regressor = SimplifiedBoostingRegressor()    \n",
    "boosting_regressor.fit(DecisionTreeRegressor, X, y, 100, 0.5, 10)\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ecde34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import make_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06110580",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplifiedBoostingRegressor:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    @staticmethod\n",
    "    def loss(targets, predictions):\n",
    "        loss = np.mean((targets - predictions)**2)\n",
    "        return loss\n",
    "    \n",
    "    @staticmethod\n",
    "    def loss_gradients(targets, predictions):\n",
    "        gradients = # YOUR CODE HERE\n",
    "        assert gradients.shape == targets.shape\n",
    "        return gradients\n",
    "        \n",
    "        \n",
    "    def fit(self, model_constructor, data, targets, num_steps=10, lr=0.1, max_depth=5, verbose=False):\n",
    "        '''\n",
    "        Fit sequence of models on the provided data.\n",
    "        Model constructor with no parameters (and with no ()) is passed to this function.\n",
    "        If \n",
    "        \n",
    "        example:\n",
    "        \n",
    "        boosting_regressor = SimplifiedBoostingRegressor()    \n",
    "        boosting_regressor.fit(DecisionTreeRegressor, X, y, 100, 0.5, 10)\n",
    "        '''\n",
    "        new_targets = targets\n",
    "        self.models_list = []\n",
    "        self.lr = lr\n",
    "        self.loss_log = []\n",
    "        for step in range(num_steps):\n",
    "            try:\n",
    "                model = model_constructor(max_depth=max_depth)\n",
    "            except TypeError:\n",
    "                print('max_depth keyword is not found. Ignoring')\n",
    "                model = model_constructor()\n",
    "            self.models_list.append(model.fit(data, new_targets))\n",
    "            predictions = self.predict(data)\n",
    "            self.loss_log.append(self.loss(targets, predictions))\n",
    "            gradients = self.loss_gradients(targets, predictions)\n",
    "            new_targets = # YOUR CODE HERE\n",
    "        if verbose:\n",
    "            print('Finished! Loss=', self.loss_log[-1])\n",
    "        return self\n",
    "            \n",
    "    def predict(self, data):\n",
    "        predictions = np.zeros(len(data))\n",
    "        for model in self.models_list:\n",
    "            predictions += # YOUR CODE HERE\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfa174f",
   "metadata": {},
   "source": [
    "### Local tests:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54221c2",
   "metadata": {},
   "source": [
    "#### Overfitting tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c94a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in tqdm(range(10)):\n",
    "    X = np.random.randn(200, 10)\n",
    "    y = np.random.normal(0, 1, X.shape[0])\n",
    "    boosting_regressor = SimplifiedBoostingRegressor()    \n",
    "    boosting_regressor.fit(DecisionTreeRegressor, X, y, 100, 0.5, 10)\n",
    "    assert boosting_regressor.loss_log[-1] < 1e-6, 'Boosting should overfit with many deep trees on simple data!'\n",
    "    assert boosting_regressor.loss_log[0] > 1e-2, 'First tree loos should be not to low!'    \n",
    "print('Overfitting tests done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e5cfd7",
   "metadata": {},
   "source": [
    "#### Zero lr tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e60fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in tqdm(range(10)):\n",
    "    X = np.random.randn(200, 10)\n",
    "    y = np.random.normal(0, 1, X.shape[0])\n",
    "    boosting_regressor = SimplifiedBoostingRegressor()    \n",
    "    boosting_regressor.fit(DecisionTreeRegressor, X, y, 10, 0., 10)\n",
    "    predictions = boosting_regressor.predict(X)\n",
    "    assert all(predictions == 0), 'With zero weight model should predict constant values!'\n",
    "    assert boosting_regressor.loss_log[-1] == boosting_regressor.loss_log[0], 'With zero weight model should not learn anything new!'\n",
    "print('Zero lr tests done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2faafe",
   "metadata": {},
   "source": [
    "#### Fitting test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2710d9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in tqdm(range(10)):\n",
    "    data, targets = make_regression(1000, 10)\n",
    "    indices = np.arange(len(data))\n",
    "    np.random.shuffle(indices)\n",
    "    data_train, targets_train = data[indices[:700]], targets[indices[:700]]\n",
    "    data_val, targets_val = data[indices[700:]], targets[indices[700:]]\n",
    "\n",
    "\n",
    "    train_loss_log = []\n",
    "    val_loss_log = []\n",
    "    for depth in range(1, 25):\n",
    "        boosting_regressor = SimplifiedBoostingRegressor()    \n",
    "\n",
    "        boosting_regressor.fit(DecisionTreeRegressor, data_train, targets_train, depth, 0.2, 5)\n",
    "        predictions_train = boosting_regressor.predict(data_train)\n",
    "        predictions_val = boosting_regressor.predict(data_val)\n",
    "        train_loss_log.append(np.mean((predictions_train-targets_train)**2))\n",
    "        val_loss_log.append(np.mean((predictions_val-targets_val)**2))\n",
    "        \n",
    "    assert train_loss_log[-2] > train_loss_log[-1] and abs(train_loss_log[-2]/train_loss_log[-1]) < 2, '{}, {}'.format(train_loss_log[-2], train_loss_log[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eedf99c",
   "metadata": {},
   "source": [
    "Here is your convergence plot from the last run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bae7383",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1, len(train_loss_log)+1), train_loss_log, label='train')\n",
    "plt.plot(range(1, len(val_loss_log)+1), val_loss_log, label='val')\n",
    "plt.xlabel('Ensemble size')\n",
    "plt.ylabel('Error')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9535cb6d",
   "metadata": {},
   "source": [
    "Great job! Please, submit your solution to the grading system!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py3 Research",
   "language": "python",
   "name": "py3_research"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
