{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VqEpGyyyGE1Z",
    "tags": [
     "pdf-title"
    ]
   },
   "source": [
    "## Задача №3\n",
    "\n",
    "В данном задании вам необходимо реализовать функции ошибки для линейной регрессии и их производные по параметрам, __не используя автоматические дифференцирование.__ Все методы должны быть реализованы только с использованием библиотеки `numpy`. \n",
    "\n",
    "Ваша основная задача: вывести формулы для производных __MSE, MAE, L1 и L2 регуляризационных членов__ в _векторном случае_ (т.е. когда и объект $\\mathbf{x}_i$, и целевое значение $\\mathbf{y}_i$ являются векторами.\n",
    "\n",
    "\n",
    "Для работы обратимся к [Boston housing prices dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html). Он был предобработан для вашего удобства и будет загружен ниже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nIf you are using Google Colab, uncomment the next line to download `boston_subset.json`\\n'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "If you are using Google Colab, uncomment the next line to download `boston_subset.json`\n",
    "\"\"\"\n",
    "# !wget https://raw.githubusercontent.com/girafe-ai/ml-course/23f_yandex_ml_trainings/homeworks/assignment03_derivatives/boston_subset.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8lQUR89nGE1f"
   },
   "outputs": [],
   "source": [
    "# Run some setup code for this notebook.\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OGf3ShTNGE1q"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"boston_subset.json\", \"r\") as iofile:\n",
    "    dataset = json.load(iofile)\n",
    "feature_matrix = np.array(dataset[\"data\"])\n",
    "targets = np.array(dataset[\"target\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WbBc_5FhGE2B"
   },
   "source": [
    "## Имплементация функций потерь и методов регуляризации.\n",
    "Для того, чтобы решить задание, вам необходимо реализовать все методы в файле `loss_and_derivatives.py`. Для вашего удобства код скопирован внутрь ноутбука в следующую ячейку. После решения ноутбука можете просто перенести его в .py файл.\n",
    "__Внимание, в данном задании не требуется использовать свободный член (bias term)__, т.е. линейная модель примет простой вид\n",
    "$$\n",
    "\\hat{\\mathbf{y}} = XW\n",
    "$$\n",
    "Единичный столбец также не добавляется к матрице $X$.\n",
    "\n",
    "Реализуйте методы для MSE, MAE, L1 и L2 регуляризации, а также вычисления их производных (опциональное задание) по параметрам линейной модели.\n",
    "\n",
    "__Для вашего удобства данные уже предобработаны, и использование линейной модели без свободного члена не является ошибкой. В данном задании он не должен быть использован.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dtELlRTOGE2E",
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class LossAndDerivatives:\n",
    "    @staticmethod\n",
    "    def mse(X, Y, w):\n",
    "        \"\"\"\n",
    "        X : numpy array of shape (`n_observations`, `n_features`)\n",
    "        Y : numpy array of shape (`n_observations`, `target_dimentionality`) or (`n_observations`,)\n",
    "        w : numpy array of shape (`n_features`, `target_dimentionality`) or (`n_features`,)\n",
    "\n",
    "        Return : float\n",
    "            single number with MSE value of linear model (X.dot(w)) with no bias term\n",
    "            on the selected dataset.\n",
    "\n",
    "        Comment: If Y is two-dimentional, average the error over both dimentions.\n",
    "        \"\"\"\n",
    "\n",
    "        return np.mean((X.dot(w) - Y) ** 2)\n",
    "\n",
    "    @staticmethod\n",
    "    def mae(X, Y, w):\n",
    "        \"\"\"\n",
    "        X : numpy array of shape (`n_observations`, `n_features`)\n",
    "        Y : numpy array of shape (`n_observations`, `target_dimentionality`) or (`n_observations`,)\n",
    "        w : numpy array of shape (`n_features`, `target_dimentionality`) or (`n_features`,)\n",
    "\n",
    "        Return: float\n",
    "            single number with MAE value of linear model (X.dot(w)) with no bias term\n",
    "            on the selected dataset.\n",
    "\n",
    "        Comment: If Y is two-dimentional, average the error over both dimentions.\n",
    "        \"\"\"\n",
    "\n",
    "        return np.mean(abs(X.dot(w) - Y))\n",
    "\n",
    "    @staticmethod\n",
    "    def l2_reg(w):\n",
    "        \"\"\"\n",
    "        w : numpy array of shape (`n_features`, `target_dimentionality`) or (`n_features`,)\n",
    "\n",
    "        Return: float\n",
    "            single number with sum of squared elements of the weight matrix ( \\sum_{ij} w_{ij}^2 )\n",
    "\n",
    "        Computes the L2 regularization term for the weight matrix w.\n",
    "        \"\"\"\n",
    "\n",
    "        return np.sum(w**2)\n",
    "\n",
    "    @staticmethod\n",
    "    def l1_reg(w):\n",
    "        \"\"\"\n",
    "        w : numpy array of shape (`n_features`, `target_dimentionality`)\n",
    "\n",
    "        Return : float\n",
    "            single number with sum of the absolute values of the weight matrix ( \\sum_{ij} |w_{ij}| )\n",
    "\n",
    "        Computes the L1 regularization term for the weight matrix w.\n",
    "        \"\"\"\n",
    "\n",
    "        return np.sum(abs(w))\n",
    "\n",
    "    @staticmethod\n",
    "    def no_reg(w):\n",
    "        \"\"\"\n",
    "        Simply ignores the regularization\n",
    "        \"\"\"\n",
    "        return 0.0\n",
    "\n",
    "    @staticmethod\n",
    "    def mse_derivative(X, Y, w):\n",
    "        \"\"\"\n",
    "        X : numpy array of shape (`n_observations`, `n_features`)\n",
    "        Y : numpy array of shape (`n_observations`, `target_dimentionality`) or (`n_observations`,)\n",
    "        w : numpy array of shape (`n_features`, `target_dimentionality`) or (`n_features`,)\n",
    "\n",
    "        Return : numpy array of same shape as `w`\n",
    "\n",
    "        Computes the MSE derivative for linear regression (X.dot(w)) with no bias term\n",
    "        w.r.t. w weight matrix.\n",
    "\n",
    "        Please mention, that in case `target_dimentionality` > 1 the error is averaged along this\n",
    "        dimension as well, so you need to consider that fact in derivative implementation.\n",
    "        \"\"\"\n",
    "\n",
    "        # Что просят: \n",
    "        # 1) вычислить производную ||Xw - Y||^2 по норме 2 \n",
    "        # 2) усреднить по n_observations и target_dimentionality\n",
    "\n",
    "        derivative = 2 * (X.T) @ (X@w - Y)\n",
    "\n",
    "        # А по факту Y.size = n_observations * target_dimentionality\n",
    "        # n_observations = X.shape[0]\n",
    "        # target_dimentionality = Y.shape[1]\n",
    "\n",
    "        # return (derivative / (n_observations * target_dimentionality))\n",
    "        return (derivative / (Y.size))\n",
    "\n",
    "    @staticmethod\n",
    "    def mae_derivative(X, Y, w):\n",
    "        \"\"\"\n",
    "        X : numpy array of shape (`n_observations`, `n_features`)\n",
    "        Y : numpy array of shape (`n_observations`, `target_dimentionality`) or (`n_observations`,)\n",
    "        w : numpy array of shape (`n_features`, `target_dimentionality`) or (`n_features`,)\n",
    "\n",
    "        Return : numpy array of same shape as `w`\n",
    "\n",
    "        Computes the MAE derivative for linear regression (X.dot(w)) with no bias term\n",
    "        w.r.t. w weight matrix.\n",
    "\n",
    "        Please mention, that in case `target_dimentionality` > 1 the error is averaged along this\n",
    "        dimension as well, so you need to consider that fact in derivative implementation.\n",
    "        \"\"\"\n",
    "\n",
    "        # Что просят: \n",
    "        # 1) вычислить производную ||Xw - Y||^2 по норме 2 \n",
    "        # 2) усреднить по n_observations и target_dimentionality\n",
    "\n",
    "        # Очень хочется сделать sign(X@w - Y) @ X, однако разные размерности => домножаем в начале на X.T\n",
    "        derivative = X.T @ np.sign((X@w - Y))\n",
    "\n",
    "        # А по факту Y.size = n_observations * target_dimentionality\n",
    "        # n_observations = X.shape[0]\n",
    "        # target_dimentionality = Y.shape[1]\n",
    "\n",
    "        return (derivative / (Y.size))\n",
    "\n",
    "    @staticmethod\n",
    "    def l2_reg_derivative(w):\n",
    "        \"\"\"\n",
    "        w : numpy array of shape (`n_features`, `target_dimentionality`) or (`n_features`,)\n",
    "\n",
    "        Return : numpy array of same shape as `w`\n",
    "\n",
    "        Computes the L2 regularization term derivative w.r.t. the weight matrix w.\n",
    "        \"\"\"\n",
    "        # Собственно, это обычный dx^2/dx = 2x\n",
    "\n",
    "        return 2*w\n",
    "\n",
    "    @staticmethod\n",
    "    def l1_reg_derivative(w):\n",
    "        \"\"\"\n",
    "        Y : numpy array of shape (`n_observations`, `target_dimentionality`) or (`n_observations`,)\n",
    "        w : numpy array of shape (`n_features`, `target_dimentionality`) or (`n_features`,)\n",
    "\n",
    "        Return : numpy array of same shape as `w`\n",
    "\n",
    "        Computes the L1 regularization term derivative w.r.t. the weight matrix w.\n",
    "        \"\"\"\n",
    "        # Просто нужен знак        \n",
    "        \n",
    "        return np.sign(w)\n",
    "\n",
    "    @staticmethod\n",
    "    def no_reg_derivative(w):\n",
    "        \"\"\"\n",
    "        Simply ignores the derivative\n",
    "        \"\"\"\n",
    "        return np.zeros_like(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обращаем ваше внимание, требуется реализовать решение в векторном виде (т.е. для каждого объекта предсказание $\\hat{\\mathbf{y}}$ является вектором с размерностью $\\geq 1$.\n",
    "\n",
    "__Внимание! При подсчете ошибки она усредняется как по объектам, так и по размерности y. Аналогичное верно и для производных__.\n",
    "\n",
    "Например, для вектора отклонений на одном объекте $[1., 1., 1., 1.]$ значение функции ошибки будет равно $\\frac{1}{4}(1. + 1. + 1. + 1.)$ \n",
    "\n",
    "Для вашего удобства метод `.mse` уже реализован и вы можете обращаться к нему за примером."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sMN81aYyGE2T"
   },
   "source": [
    "Для проверки своего кода вам доступно несколько assert'ов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KKUYnPWuGE2V"
   },
   "outputs": [],
   "source": [
    "w = np.array([1.0, 1.0])\n",
    "x_n, y_n = feature_matrix, targets\n",
    "\n",
    "# Repeating data to make everything multi-dimentional\n",
    "w = np.vstack(\n",
    "    [w[None, :] + 0.27, w[None, :] + 0.22, w[None, :] + 0.45, w[None, :] + 0.1]\n",
    ").T\n",
    "y_n = np.hstack([y_n[:, None], 2 * y_n[:, None], 3 * y_n[:, None], 4 * y_n[:, None]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 477
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1344,
     "status": "error",
     "timestamp": 1582397124081,
     "user": {
      "displayName": "Victor Yacovlev",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDahDnBQR6_kQQX4xt7llKTI0xt2Z802bvVR4MrqA=s64",
      "userId": "11689260236152306260"
     },
     "user_tz": -180
    },
    "id": "UtkO4hWYGE2c",
    "outputId": "cb0b99a8-2db4-4873-dfd8-741b52db29f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE derivative:\n",
      "[[ 7.32890068 12.88731311 18.82128365 23.97731238]\n",
      " [ 9.55674399 17.05397661 24.98807528 32.01723714]] \n",
      "\n",
      "L2 reg derivative:\n",
      "[[2.54 2.44 2.9  2.2 ]\n",
      " [2.54 2.44 2.9  2.2 ]]\n"
     ]
    }
   ],
   "source": [
    "reference_mse_derivative = np.array(\n",
    "    [\n",
    "        [7.32890068, 12.88731311, 18.82128365, 23.97731238],\n",
    "        [9.55674399, 17.05397661, 24.98807528, 32.01723714],\n",
    "    ]\n",
    ")\n",
    "reference_l2_reg_derivative = np.array([[2.54, 2.44, 2.9, 2.2], [2.54, 2.44, 2.9, 2.2]])\n",
    "\n",
    "assert np.allclose(\n",
    "    reference_mse_derivative, LossAndDerivatives.mse_derivative(x_n, y_n, w), rtol=1e-3\n",
    "), \"Something wrong with MSE derivative\"\n",
    "\n",
    "assert np.allclose(\n",
    "    reference_l2_reg_derivative, LossAndDerivatives.l2_reg_derivative(w), rtol=1e-3\n",
    "), \"Something wrong with L2 reg derivative\"\n",
    "\n",
    "print(\n",
    "    \"MSE derivative:\\n{} \\n\\nL2 reg derivative:\\n{}\".format(\n",
    "        LossAndDerivatives.mse_derivative(x_n, y_n, w),\n",
    "        LossAndDerivatives.l2_reg_derivative(w),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE derivative:\n",
      "[[0.19708867 0.19621798 0.19621798 0.19572906]\n",
      " [0.25574138 0.25524507 0.25524507 0.25406404]] \n",
      "\n",
      "L1 reg derivative:\n",
      "[[1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "reference_mae_derivative = np.array(\n",
    "    [\n",
    "        [0.19708867, 0.19621798, 0.19621798, 0.19572906],\n",
    "        [0.25574138, 0.25524507, 0.25524507, 0.25406404],\n",
    "    ]\n",
    ")\n",
    "reference_l1_reg_derivative = np.array([[1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0]])\n",
    "\n",
    "assert np.allclose(\n",
    "    reference_mae_derivative, LossAndDerivatives.mae_derivative(x_n, y_n, w), rtol=1e-3\n",
    "), \"Something wrong with MAE derivative\"\n",
    "\n",
    "assert np.allclose(\n",
    "    reference_l1_reg_derivative, LossAndDerivatives.l1_reg_derivative(w), rtol=1e-3\n",
    "), \"Something wrong with L1 reg derivative\"\n",
    "\n",
    "print(\n",
    "    \"MAE derivative:\\n{} \\n\\nL1 reg derivative:\\n{}\".format(\n",
    "        LossAndDerivatives.mae_derivative(x_n, y_n, w),\n",
    "        LossAndDerivatives.l1_reg_derivative(w),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kJcSPj8UGE20"
   },
   "source": [
    "### Градиентный спуск для решения реальной задачи\n",
    "Следующая функция позволяет найти оптимальные значения параметров с помощью градиентного спуска:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "On6aSWuIGE21"
   },
   "outputs": [],
   "source": [
    "def get_w_by_grad(\n",
    "    X, Y, w_0, loss_mode=\"mse\", reg_mode=None, lr=0.05, n_steps=100, reg_coeff=0.05\n",
    "):\n",
    "    if loss_mode == \"mse\":\n",
    "        loss_function = LossAndDerivatives.mse\n",
    "        loss_derivative = LossAndDerivatives.mse_derivative\n",
    "    elif loss_mode == \"mae\":\n",
    "        loss_function = LossAndDerivatives.mae\n",
    "        loss_derivative = LossAndDerivatives.mae_derivative\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"Unknown loss function. Available loss functions: `mse`, `mae`\"\n",
    "        )\n",
    "\n",
    "    if reg_mode is None:\n",
    "        reg_function = LossAndDerivatives.no_reg\n",
    "        reg_derivative = (\n",
    "            LossAndDerivatives.no_reg_derivative\n",
    "        )  # lambda w: np.zeros_like(w)\n",
    "    elif reg_mode == \"l2\":\n",
    "        reg_function = LossAndDerivatives.l2_reg\n",
    "        reg_derivative = LossAndDerivatives.l2_reg_derivative\n",
    "    elif reg_mode == \"l1\":\n",
    "        reg_function = LossAndDerivatives.l1_reg\n",
    "        reg_derivative = LossAndDerivatives.l1_reg_derivative\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"Unknown regularization mode. Available modes: `l1`, `l2`, None\"\n",
    "        )\n",
    "\n",
    "    w = w_0.copy()\n",
    "\n",
    "    for i in range(n_steps):\n",
    "        empirical_risk = loss_function(X, Y, w) + reg_coeff * reg_function(w)\n",
    "        gradient = loss_derivative(X, Y, w) + reg_coeff * reg_derivative(w)\n",
    "        gradient_norm = np.linalg.norm(gradient)\n",
    "        if gradient_norm > 5.0:\n",
    "            gradient = gradient / gradient_norm * 5.0\n",
    "        w -= lr * gradient\n",
    "\n",
    "        if i % 25 == 0:\n",
    "            print(\n",
    "                \"Step={}, loss={},\\ngradient values={}\\n\".format(\n",
    "                    i, empirical_risk, gradient\n",
    "                )\n",
    "            )\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим простой пример:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A1pyDIyqGE25"
   },
   "outputs": [],
   "source": [
    "# Initial weight matrix\n",
    "w = np.ones((2, 1), dtype=float)\n",
    "y_n = targets[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "erTRQiAFGE29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step=0, loss=231.28353984777308,\n",
      "gradient values=[[3.03360308]\n",
      " [3.97457575]]\n",
      "\n",
      "Step=25, loss=58.512903511682715,\n",
      "gradient values=[[2.28551977]\n",
      " [4.44706638]]\n",
      "\n",
      "Step=50, loss=48.29584498872882,\n",
      "gradient values=[[-0.89558132]\n",
      " [ 0.76425616]]\n",
      "\n",
      "Step=75, loss=47.292783042717005,\n",
      "gradient values=[[-0.48111511]\n",
      " [ 0.40907079]]\n",
      "\n",
      "Step=100, loss=47.00419092029711,\n",
      "gradient values=[[-0.25806412]\n",
      " [ 0.21942022]]\n",
      "\n",
      "Step=125, loss=46.921159712801064,\n",
      "gradient values=[[-0.1384223 ]\n",
      " [ 0.11769421]]\n",
      "\n",
      "Step=150, loss=46.897270698227686,\n",
      "gradient values=[[-0.07424796]\n",
      " [ 0.06312967]]\n",
      "\n",
      "Step=175, loss=46.890397559386315,\n",
      "gradient values=[[-0.03982566]\n",
      " [ 0.03386195]]\n",
      "\n",
      "Step=200, loss=46.88842007984702,\n",
      "gradient values=[[-0.02136197]\n",
      " [ 0.01816312]]\n",
      "\n",
      "Step=225, loss=46.88785113668749,\n",
      "gradient values=[[-0.01145829]\n",
      " [ 0.00974247]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w_grad = get_w_by_grad(x_n, y_n, w, loss_mode=\"mse\", reg_mode=\"l2\", n_steps=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сравнение с `sklearn`\n",
    "Сравним реализованную модель с версией из `sklearn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn linear regression implementation delivers MSE = 42.53541245128315\n"
     ]
    }
   ],
   "source": [
    "lr = Ridge(alpha=0.05)\n",
    "lr.fit(x_n, y_n)\n",
    "print(\n",
    "    \"sklearn linear regression implementation delivers MSE = {}\".format(\n",
    "        np.mean((lr.predict(x_n) - y_n) ** 2)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gse1m4nyGE3C"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXkAAAD4CAYAAAAJmJb0AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAAsTAAALEwEAmpwYAABD9UlEQVR4nO2de3gU1fnHP2eXQBJAwiUoBAhgAeUaIFwUpHgDKwp4QYSg4qVWsdbaSsViK1r8SYutVSsg3i0RoUoRFQtFpSKIQiDIRRBEAgRUQINAAiSb8/tjdpfNZmZ39p4N7+d58pCdOTNzdsl+5533vBeltUYQBEGonTgSPQFBEAQhdojIC4Ig1GJE5AVBEGoxIvKCIAi1GBF5QRCEWkydRE/Al2bNmum2bdsmehqCIAhJRUFBwUGtdabZvhol8m3btmXt2rWJnoYgCEJSoZQqston7hpBEIRajIi8IAhCLUZEXhAEoRZTo3zyghAp5eXl7N27l+PHjyd6KoIQdVJTU2nVqhUpKSm2jxGRF2oVe/fupWHDhrRt2xalVKKnIwhRQ2vNoUOH2Lt3L+3atbN9XNKL/ML1xUxfso3ikjKcSuHSmqyMNCYO7cTInllVxv1+weeUlldWO0eKA1waKjU4lWJMv9ZMHdnN8twXnpPJh1sPsK+kjJYm1wpn/nbPZTU+1PPUVo4fPy4CL9RKlFI0bdqUAwcOhHRcUov8wvXFPLBgI2XlLgBc7oqaxSVlPLBgI4BXAH8zv5BKi4Kbvrrv0po5q3fz9YGjrNt92PTcc1bv9o73v1Yk8w92Lqvxa4u+582CYtvnqe2IwAu1lXD+tpN64XX6km1eYfOnrNzF9CXbvOOsBN6KlV99b3nuQNcKBbP5BzqX1fi5n+4J6TyCIJw+JLXI7ysps7U/2Lh4zCWUY0Ld7rLoCRCP953UHN0JaybAv7PgNYfx75oJxvYIaNCgQZXXL7/8Mr/85S8jOqeHKVOm8Pjjj4d17KJFi5g2bRoACxcuZMuWLVXmuG/fvqjMcdeuXXTt2hWAtWvX8qtf/Soq501m/vjHP7Js2bKEXDupRb5lRpqt/cHGxWMuoRwT6nanxSNcPN530lK8GN7tCttnQtk+QBv/bp9pbC9enOgZRpWKigqGDx/OpEmTgNiKvC+5ubk89dRTUT+vLy6X9RN3RUVFROeO9HgPjzzyCJdccklUzhUqSS3yE4d2Ii3FabovLcXJxKGdvOMcIbqyBpzdxPLcga61cH0xA6Z9QLtJ7zJg2gcsXF8c0vx9z2V3/Jh+rUM6z2nP0Z3w8bXgsnjScZUZ+yO06M14++236devHz179uSSSy7h22+/BQwL/ZZbbmHw4MG0b9++ijA++uijdOzYkYEDB7Jtm+GC++677+jduzcAGzZsQCnF7t3GWtHZZ59NaWkp48eP54477qBfv3787ne/8z5RrFq1ikWLFjFx4kRycnL485//zNq1a8nLyyMnJ4eysjIKCgr46U9/Su/evRk6dCj79+8HYPDgwdx///307duXjh07smLFioDvd/ny5VxxxRVB3+OcOXPo27cvOTk5/OIXv/AK95133klubi5dunThoYce8o5v27Yt999/P7169eJf//pXlWv6v++vvvqKyy67jN69e3PBBRewdetWAL766iv69+9Pt27dePDBB71PYMuXL+eCCy5g+PDhdO7cGZfLxcSJE+nTpw/du3fn2WefBWD//v0MGjSInJwcunbtyooVK3C5XIwfP56uXbvSrVs3nnjiCe+c3njjDQDef/99evbsSbdu3bjllls4ceKE9z099NBD9OrVi27dunnnGSlJvfDqWVQMFl3j+TdQdE1FJXicHukpDkbltmFUrvm5raJrQl1I9Z2/naiYQONzs5tIdI1dvnjcWuA9uMqMcX1mhHz6srIycnJyvK+///57hg8fDsDAgQNZvXo1Simef/55/vKXv/DXv/4VgK1bt/Lhhx9y5MgROnXqxJ133snnn3/O66+/TmFhIRUVFfTq1YvevXvTvHlzjh8/zo8//siKFSvIzc1lxYoVDBw4kObNm5Oeng4YIaWrVq3C6XTy8ssvA3D++eczfPhwrrjiCq699loA3nvvPR5//HFyc3MpLy/n7rvv5q233iIzM5N58+YxefJkXnzxRcCwbj/77DMWL17Mww8/HJIbwuw97tixg3nz5rFy5UpSUlKYMGEC+fn53HjjjTz66KM0adIEl8vFxRdfzOeff0737t0BaNq0KevWrTO9ju/7vvjii5k1axYdOnTg008/ZcKECXzwwQfcc8893HPPPYwZM4ZZs2ZVOX7dunVs2rSJdu3aMXv2bBo1asSaNWs4ceIEAwYMYMiQISxYsIChQ4cyefJkXC4XpaWlFBYWUlxczKZNmwAoKSmpct7jx48zfvx43n//fTp27MiNN97IzJkz+fWvfw1As2bNWLduHTNmzODxxx/n+eeft/3ZWpHUIg+G8NkRs0Dj/MW5tLySe+cVkte/DSsnXWR7LoEWUgMJdyhibDU+1POc1ux9y/64MEQ+LS2NwsJC7+uXX37ZW3hv7969jB49mv3793Py5Mkq8c7Dhg2jXr161KtXj+bNm/Ptt9+yYsUKrrrqKq9oe24WYIj1ypUr+eijj/j973/Pf/7zH7TWXHDBBd4xo0aNwum090TqYdu2bWzatIlLL70UMNwhLVq08O6/+uqrAejduze7du0K6dxm7/H999+noKCAPn36AMZNsnnz5gDMnz+f2bNnU1FRwf79+9myZYtX5EePHm15Hc/7Pnr0KKtWrWLUqFHefR7L+ZNPPmHhwoUAjB07lvvuu887pm/fvt7/m6VLl/L55597LfHDhw+zfft2+vTpwy233EJ5eTkjR44kJyeH9u3bs3PnTu6++26GDRvGkCFDqn227dq1o2PHjgDcdNNNPPPMM16R9/1sFyxYENJna0XSi3w0MBNnDeSv3k1udhPb4hnqgqmQIMr22xt3/JuoX/ruu+/mN7/5DcOHD2f58uVMmTLFu69evXre351OZ1B/8KBBg1ixYgVFRUWMGDGCP//5zyilGDZsmHdM/fr1Q56j1pouXbrwySefmO73zNPOHK2O9T1ea81NN93EY489VmXs119/zeOPP86aNWto3Lgx48ePr5LJHOi9efZVVlaSkZFR5aZrB99za615+umnGTp0aLVxH330Ee+++y7jx4/nN7/5DTfeeCMbNmxgyZIlzJo1i/nz53ufgOwQyWdrRVL75KOFlQhrCCkMMdQFUyFBpLUIPgYg9ayoX/rw4cNkZRlGwyuvvBJ0/KBBg1i4cCFlZWUcOXKEt99+27vvggsuYM6cOXTo0AGHw0GTJk1YvHgxAwcODHrehg0bcuTIEdPXnTp14sCBA16RLy8vZ/PmzSG9z1C4+OKLeeONN/juu+8Aw71VVFTEjz/+SP369WnUqBHffvst7733XsjnPuOMM2jXrp3Xb6+1ZsOGDQD079+fN998E4DXX3/d8hxDhw5l5syZlJeXA/Dll19y7NgxioqKOPPMM/n5z3/Obbfdxrp16zh48CCVlZVcc801TJ06tZo7qVOnTuzatYsdO3YA8M9//pOf/vSnIb+vUBCRJ7AIh2KFh7qQKiSIViOiOy4EpkyZwqhRo+jduzfNmjULOr5Xr16MHj2aHj168LOf/czr0gBjoU5rzaBBgwDD35+RkUHjxo2Dnvf6669n+vTp9OzZk6+++sq7WJmTk4PL5eKNN97g/vvvp0ePHuTk5LBq1arw33QQOnfuzNSpUxkyZAjdu3fn0ksvZf/+/fTo0YOePXtyzjnnMHbsWAYMGBDW+fPz83nhhRfo0aMHXbp04a23DHfd3//+d/72t7/RvXt3duzYQaNGjUyPv+222+jcuTO9evWia9eu/OIXv6CiooLly5d75zhv3jzuueceiouLGTx4MDk5OYwbN67a00lqaiovvfQSo0aNolu3bjgcDu64446w3pddlLaIsU4Eubm5OhFNQxauL+beeYWYfRJZGWkh+eWlvEBi+eKLLzj33HMDDzq60wiTDLT46kyDYZuhgf0aIUJyUVpaSlpaGkopXn/9debOneu9AdRkzP7GlVIFWutcs/Hik8dYtFxb9D35q3dXEfpwrHBZAE0CGrSHgW9Yh1E604z9IvC1moKCAn75y1+itSYjIyMk33kyISLvZurIbgDM/XQPLq1xKsU1vUWway1Zl8OwTUaY5N63jEXW1LMMF825E0XgTwMuuOACr3++NiMi72bh+mLeLCj2lghwac2bBcUhRdcISUaD9kaIZBhhkoKQLMjCq5tQi4UJgiAkA6eVJR9oUVRi3AVBqI2cNpa8J6u1uKQMzamSA57aMhLjLghCbeS0EflA7piF64spPVk9u0xi3IVo0bZtWw4ePFhtu39Z4ljz8ssvk5mZSU5ODuecc463gBbArFmzePXVV6sd41s6WEg+Tht3jZXbxWPR+98AMtJSmDK8iyy6CkmBy+WyXaNm9OjR/OMf/+DQoUN06tSJa6+9ltatW8c8KUdIDKeNJR+oFrtZB6gjxyu4d15h0HLBQnKTD7TF+CK0db+OhGPHjjFs2DB69OhB165dmTdvXpX9ZWVl/OxnP+O5556rduz06dO95Wx9y+qOHDmS3r1706VLF2bPnu3d3qBBA37729/So0cPPvnkExo0aMDkyZPp0aMH/fv395YxtqJp06b85Cc/8ZYR9m1IUlBQQI8ePejRowfPPPOM95jS0lKuu+46OnfuzFVXXUW/fv28xdeWLl3KeeedR69evRg1ahRHjx4N8dMTYsFpI/JWJQesuiq5tDb13Qu1h3zgdqAIo05Rkft1JEL/n//8h5YtW7JhwwY2bdrEZZdd5t139OhRrrzySsaMGcPPf/7zKsctXbqU7du389lnn1FYWEhBQQEfffQRAC+++CIFBQWsXbuWp556ikOHDgHGDaVfv35s2LCBgQMHcuzYMfr378+GDRsYNGiQ6Y3El927d3P8+HFvVUdfbr75Zp5++ulqceQzZsygcePGbNmyhT/96U8UFBQAcPDgQaZOncqyZctYt24dubm5/O1vfwv9AxSizmkj8iN7ZvHY1d3IykhDYZQr8LwOhoRS1k4mA6V+20rd28OlW7du/Pe//+X+++9nxYoVVeqhjBgxgptvvpkbb7yx2nFLly5l6dKl9OzZk169erF161a2b98OwFNPPeW1zvfs2ePd7nQ6ueaaa7znqFu3rrdBR6AywPPmzaN79+785Cc/YcKECaSmplbZX1JSQklJibcmzg033ODd9/HHH3P99dcD0LVrV+8NYvXq1WzZsoUBAwaQk5PDK6+8QlFRUUifnRAbThuRB0PoJw7tRMuMNPaVlDF9yTYuPCfTVgcoCaWsfewOcbsdOnbsyLp167zdhh555BHvvgEDBnhrvvujteaBBx6gsLCQwsJCduzYwa233sry5ctZtmwZn3zyCRs2bKBnz57ecrupqalV/PApKSkodyvIQKVqR48ezeeff86qVauYNGkS33wTeUllrTWXXnqpd/5btmzhhRdeiPi8QuScViJvFkY5b80e8KlYY9UmUEIpax9tQtxuh3379pGens64ceOYOHFilVKzjzzyCI0bN+auu+6qdtzQoUN58cUXvX7s4uJivvvuOw4fPkzjxo1JT09n69atrF69OoLZVSU3N5cbbriBJ598ssr2jIwMMjIy+PjjjwGjiqOHAQMGMH/+fAC2bNnCxo1G57P+/fuzcuVKbwndY8eO8eWXX0ZtrkL4REXklVIvKqW+U0pt8tnWRCn1X6XUdve/weufxhizMMpyl6bMpyWgUylSnFWVXkIpayePAul+29Ld28Nl48aN3l6lDz/8MA8++GCV/U8++SRlZWX87ne/q7J9yJAhjB07lvPOO49u3bpx7bXXcuTIES677DIqKio499xzmTRpEv37949gdtW5//77eemll6rUlgd46aWXuOuuu8jJyany5DFhwgQOHDhA586defDBB+nSpQuNGjUiMzOTl19+mTFjxtC9e3fOO++8qPUoFSIjKqWGlVKDgKPAq1rrru5tfwG+11pPU0pNAhprre8PdJ5YlxpuO+ldW+My0lKoX6+OlAtOQmyVGvYhH8MHvxvDgn8UyIvN1GoFLpeL8vJyUlNT+eqrr7jkkkvYtm0bdevWTfTUThsSUmpYa/2RUqqt3+YRwGD3768Ay4GAIh9rPM24g3G4rJzCh4YEHRcpUns+8eQhoh4KpaWlXHjhhZSXl6O1ZsaMGSLwNZxYJkOdqbX2NNP8BjgzhteyhR2Bh/j43/2bh3tCNQEReqHG0rBhQxLR2EcIn7gsvGrDJ2SqsEqp25VSa5VSaw8cOBDTedgJlwS48JzMmM4DpOplLKlJ3c4EIZqE87cdS5H/VinVAsD973dmg7TWs7XWuVrr3MzM2IrrxKGdqi2qmvHh1tjebECqXsaK1NRUDh06JEIv1Dq01hw6dKhaXkMwYumuWQTcBExz/1szmifa+O7HQ2hbZqRRbHIdCdWMjFatWrF3715i/VQoCIkgNTWVVq1ahXRMVEReKTUXY5G1mVJqL/AQhrjPV0rdipExfl00rhUK/gubx05UUF4ZXOXjIbQTh3aqVhhNQjUjJyUlhXbtpHWfIHiIVnTNGItdF0fj/OFgtrBph3gJrWdxVaJrBEGIJbW21LDZwqYVTqWo1NpUaGMZ5jiypzQKFwQhttRakbfrV09LcfLY1d1MxVbCHAVBSHZqbe0aK7964/SUapUorQRbwhwFQUh2aq0lb7Ww+dCV9rs9SZijIAjJTq215K3qx4fiZpHm3oIgJDu11pKHyBc24xXmKDVsBEGIFbVa5CMlHmGOsrgrCEIsEZEPQqzDHAMt7orIC4IQKbXWJ58syOKuIAixRCx5E+LpI5caNoIgxBKx5P0w6wP7wIKNLFxfHJPrTRzaqVojcalhIwhCtBCR9yPeCVDRCPUUBEGwQtw1fiTCRy41bARBiBUi8n5Y+cgbpaUwYNoHEssuCEJScdq5axauL2bAtA9oN+ldBkz7oJqv3cxHnuJQHDtZETc/vSAIQrRQNalNWm5uro5lk2D/xCMwBLxBah1KSsu9FjpUTYAqPVnBD6Xl1c7nVIq/XtdDLHpBEBKKUqpAa51ruu90EvkB0z4I2jzErPRwu0nvWnYNVEBe/zZMHdktehMVBEEIgUAif1q5a+wsnppF0gSKWddA/urd4roRBKFGctqI/ML1xTiUsjXW/2Zg5qf3RUPUQyyDrR0IgiDY4bSIrvH44l02XVP+lrvHdfPb+Rssz1FcUsbC9cVB/fN2smlDLVomVSwFQbDitBB5q36vCqjjVJS7Tgm3VbapRzTvnVdo6Z8PVj0ymHh7xNps3cCqaJlUsRQEIRCnhbsmkC9++rU9bGebjuyZRV7/Nlg5fYJlxgbKpvUtpxDK+5AWhYIgBOK0sOQDFQELlm3qa107lcKlNRlpKZSUVQ+phMA3lEDZtFZPG/7zDeWcgiAIp4UlH24RMH/r2uOPtxJ4MBdizyKqlZunZUZaUFG2mq+0KBQEIRC1SuStIlLCLQJmx7r2JcWhqglxMDeMR7wDiXKg+UoVS0EQAlFr3DXBFiDtFgHzjVQJOU3MxFkf6EaR5RcJY9ZPNtjNKB4tCgVBSF5qjchHo42eWdmDUCh36WrXC7SQunLSRd7fIxFrqWIpCIIVtUbko7EAGap7xs71PIu1/jhNErNErAVBiDa1RuSj0UYvGhEp/tezSp6ym5hlF0mIEgTBjKQX+XxgMlB0/4VQqcGhcJaUkbFkG5mbvw1pAdLqRuGLwihj0Dg9haPHKyivDJxIlWVxzqwoRr9IQpQgCFbEPLpGKXWZUmqbUmqHUmpSNM+dD9wOFBkXAqcDlMLVOJ0frunOZbf2rSJydmrJpziqulEcGILuicp5YnQOu6YNY/0fhzB9VPBEqgvPyay2Hhvt6BdJiBIEwYqYWvJKKSfwDHApsBdYo5RapLXeEo3zTwZKLfZVpjj5Z9smPOF+bdva9VPkSmBY9xZMHdnN6xK5d16h1yXiu3jqYeH6Yh5+e7NpDXoFXNPbXgKWXdeLJEQJgmBFrN01fYEdWuudAEqp14ERQFREfneQ/YeAZsD3QN32TWjQ5UwaFO7z7vePvpm+ZFuVOjYe5qzezZsFe6mo1N79npvE2qLv+XDrAa8gX3hOJvPW7DE9Dxiung+3HrCccziul2isRwiCUDuJtbsmC9jj83qve5sXpdTtSqm1Sqm1Bw5Yi58ZbWyMOYQhrCcapXHo2u7s+cOlFD12OXvvv5CjOS2rWLuBLN+y8spqwl1W7iJ/9e4qbQHnrN5tKfB2rhOO60USogRBsCLhC69a69nAbDA6Q4Vy7KMYPnkrl0016jiprGOIoatxOodG5/DDlV3o+NZmyj/ZhcMi3DEQ4cTIBLKwrRZ+A90YJCFKEAQrYi3yxUBrn9et3NuiQt7RnfDtciafdSlF6a2MxddQUIrK+nXZPrwzdc9tTosXP4vW1AJy7ESFae35heuLvdE7/gRzvUiMvSAIZsRa5NcAHZRS7TDE/XpgbFTOXLwYPr6WPFcZeUB+9hhu7/ccpXXqh34upTjZoRlFj10OQP1Pimi2aHNUpmlGSVm5qZ99+pJtpgKvQFwvgiCERUx98lrrCuCXwBLgC2C+1jpy9Ty6Ez6+FlynXBh5RXOZ/enPyT62C6UraXr8AHVdJ+yfUynvz7Hzsvnmlr4RTzMQZn52K5eMRuLdBUEIj5j75LXWi4HFUT3pF49XEXgPeUVzySua632dnz2GyTn/R1F6dmiuHKU4EaFln+JQoAi4COsv6lZRMtFMnBIE4fQiOUsN733L1rC8ornseqsdd375DIRaRsDPsveNyAl4GIYoN0itEzTKxt/PnogoGWkYLgi1m4RH14RF2f6Qhs8ouBuAmR0nACqsBVo4FZFzpFcr00XarIw0b3JUu0nvBjylmXjHO0omkeUQpNaOIMQHpaNcKCsScnNz9dq1a4MP/HcWlO0LPs6ErOG72FffL8I+VNF3f2Z1tx/0ir1/7fcB0z6wDIf0ryOfKKzm6HuzigVmJZ3t1M4XBMEcpVSB1jrXdF9SivyaCbB9ZlSuWfe6Y5Q700IXejjlAnJpfr23hCfaNvHushKya3pnVcmQ9Yh9MMs2FpZvu0nvWkbzfD1tWETnDkSibi6CUFsJJPLJ6ZM/9z5wRmcx8uT8+qd89uH67es4+HvbJihggnuXWcvBa3pnMW/NnioZshPf2MCDCzd6WwR6tj+wYKPXP+7bQtBsf7gkqj+s1NoRhPiRnCLfoD0MfAMc9aJyuhkFd6PnOrh4/9LwxN6HmRiW8CUYQr9y0kV8PW0YKyddxLuf76+2GFvu0rz26e6ApQxiVWUyGgu94SzcSvNxQYgfySnyAFmXwxVboOXlmDZXDYNlyy9Dz3UwZ1UeTteJiMT+fa2rWPaAaVVKMMrgm+GxbGNl+Ybb4NxDuE8YUmtHEOJHcvrk/Tm604id3/sWHP8GUs+CMy809u16HQivpd8lg//D+y2GGC/C8dmDcaNwH6uOnqDJO1uqVMIMhMdHXVN92JHMS6JrBCF61L6F11A4uhPe7gg6/N6tVcQewhd8AK1RJWW0+fOHAYd5athkucsXv1lQXOOiURK1cCsIQlVq38JrKDRoD4MWgaNu2KfwuHH0XAedSz6PzG+vFDojjaLHLufg8C5Vdnmae/sWKSsuKePNgmKu6Z0V1K0S78Qm8a0LQs2n9lvyHo7uhLV3w77oVFjo8rNCtmR0N15E4soBb0/ahoX7ApY28LhAzFwdQNxjzyXeXRBqBqe3u8afozvZs/w+mpYsJk0ZBcwi8b5ExW8P7qcDqL/avE6OxwViJqwpTkWFS5u6TuKR2CS+dUFILIFEPjnLGkRCg/Zcv+mXFJfcCkDf+hv5U8uZdEjdg0KHrNPLll8GGMXQbjjvFbSqE57YK6Og2bHzsjl2XjaqwkWTNzd6F2k9LhCzcMpQiqBFG6ljLwg1m9rvkzfBV/g+O9aNodtn0H7j27Tf+A6M1XBx4EVRM/KK5lL5el13YlVl+H57d4KVTqnDodE5fP/r3CrhhaGKtkMpKTomCKcxp6XIB10wPHMw/PTdsJKtjMQqZ9QWaY+c2ZytjwzlH632wtGdIS9qurSOSnasIAjJyWkp8raScdzJVv88eDnflDfBpcEVol5vfi+neiZtmKUT3m/WF1W/Havuv5DdDw8NWvLYl2hkx54OSNlloTZy+i28urG7YOif8DO44RpmZ0+lriO8uPsqVTAjXKhNLz9G13fe59vPgi+tSOx6YCRSSEhmJLomAsy+/K3rfsOfsp7lpw3WhK3TUQnBBNCaO7c9w32rHmNS8d18dqyb6bBEZ8fWdGpqVrEg2EFEPkIWri/m4bc3V6s907ruN9zebAGXNlpNZp3vAcP/FYpmV6tvH0HJ41TXcZ5bfSuZn+9k6jc/Z8/Js4DEWaTJFF4p2btCMiMiHwUCNQHxZVTjpUxr9RTOMLQ6mglWF+9byr+XDefW3Q8z9oobEiLwyeT+EEteSGZO77IGUcJu6OK/fhjC4G3Ps/LHbiGvsXoWaiOKyvEs1LYcwhk3HqfZqAaM/CIbVo0zsn7jRKzKI8cKqYwp1FZE5G1iFbqYkZZClt++PSfPIm/XYwza9jz/PHg5B8ozQgquMY3KCRW32M/sdBdqTDkN+8wgf8ODML8xfLs89POFSLI1Bom07LIg1FTEXWOTYO6HheuLmfjGhirZpylORYpDUVpe6d02uOEanms7lRRlPzpnQu+nmdlhwqlOVOHi/r++88tnjObmZ14M/WYbRdyijLg/BCF+iLsmCvhbehlpKaSmOLh3XiEDpn3A2qLv8V+5K3fpKgIPsPxIHy7a9izbHBfYvvaMgrvRrzuZsyqPOq6TESVXoRQzO96FGlPJJedOhEVnw/JhUXfliPtDEGoGp70lH04EyIMLN5K/erdpNIZdGqensP43P2HnRw+TeWgBDRxHbRvp+dljuLXf85zw9LmNaJHWeBfZpbt5tPD35O3+l9Ftq/cTEVv4yRRdIwjJjETXWBBOBMjC9cXcO68wIoH3MK5/G28zEE845hUZK8hwHrGt2xN6P83MjncZLyJx5QBoTdMTB3my4B7yiuZCv+fh7FsjO6cJIv6CEF1E5C0Ix29sN5TSDkqZe1584++b1/kehw3tjlomLRgJVl8+wzNr76ZSOXH2ehzO+XVk53STbKGVgpAMiE/egnAiQKIl8GDtWt9z8iz+sG8C/b94lQ6b3oXhX7G//kUBXfHFi9p6m5A3KP8x4sJoMzvehWNsJd1/VoAuuBedr9j9SitWfPyv8M7pJtlCKwUh2TmtRT7U9nUL1xcToY0cMi6tWbi9Hhetncigbc/z7x8GU1ZZ11LD84rmcuRfjarH24dZGG1LRnccYytxjK2k7Q1FzG32Lfo1Ba8pWDIg5AXbZAutFKKDFH9LHBG5a5RSo4ApwLlAX631Wp99DwC3Ai7gV1rrJcHOV1N98h4fcjSteLtkZaRRerKiWkkFMNw6z2c/TMfUPUE9NNGslXPx/qXeZile0rJYkfkEkz5uGtDXLqGVpx/ioos9sXTXbAKuBj7yu2Bn4HqgC3AZMEMp5ax+eGKxkwDj+QNNhMCnpTi58JxMU4EHw60zdPtMBm17nk2l7QIa654Eqzmr8qhTaX4+WyjF+y2GoMZUUuf6cib0ftrYXlbMwKLr+CBrCMMa/Y/ikjLTOvYTh3Yixa/mQ4pTJSS0UqzL+CAuusQSUfs/rfUXAKq6ZTgCeF1rfQL4Wim1A+gLfBLJ9WJBsPZ1UxZtrvYHGg+Ugnp1HMxZvTvo2D0nz+KKHU/Tuu43/ObMOVx+xsfUdVSYGux5RXMBuKf3kxyq1+zUxUKdHOBSdZjZ8S6+bNiBZcsvM+bsdPF0m+k8xXS2H2/DUx/ew8iev6l6vP/NKAFr//7WpeemBIh1GWXERZdYYuWTzwL2+Lze695WDaXU7UqptUqptQcOHIjRdMJj4fpiSsoisHojQGtCvvaek2dx75776LR5odu6b2/ptz+4oHnV0gnh4rHsr3d5rXqlwKGgU9pu/nHmbw3//ZtnwpoJ5C/7kPLKqtcrr9Qxt+r8rfaH365+8xbrMjaEuvYlRJegIq+UWqaU2mTyMyIaE9Baz9Za52qtczMzM6NxyqiRzF94w7p/yrtYe7wyxXT9ddnyy5izKo/6JyOIyFEKHA5vJq0aU0mzq78jP3vMqYeEE9/B9pnMb3kDr7V7gNZ1v6lyikBWXaRuFV+Xm8aw2q1cYGJdRh/Jfk4sQUVea32J1rqryc9bAQ4rBlr7vG7l3pZUJMsX3qkU6Snm/5Ue6/6cTf+m3cZ3GL3zMVx+Op5XNJejbzTy+uyzj+2KKCIHpTiUmskN588hP3tMtSHnN9zIR51u44W2U7xiHyiiyV+gQ+1Za+YTtkKsy+gjxd8SS1SSoZRSy4H7PNE1SqkuwGsYfviWwPtAB611wG9aTStQFs3Ep5rE4IZrmJX9GKmOkwHHTej9NLM6TkATQWE0999XlUza6rspSzmL9J/ONZqo+xCNaByrhiD+SMSHkKzELONVKXUV8DSQCZQAhVrroe59k4FbgArg11rr94Kdr6aJvFnoV23Bk1V7WaOVNK1z2Bv/b6blRumECRCJ2EOVJwMr0afrFOj+UNCwVbOOTWblEgB+O38DLpO/84y0FOrXqxOT8gpSukGIJ1LWIAJ8v6wZ6SkcPV5RbeEwEtJSnDXmJuIR/hEZH9LQWVZNz/Ozx3BT/5dwOepGXjoBQFdy55czjLLHPqxp+TQ3vv+TgJ+LvyVvdkNOcShQVCn/7CGWVrvEhQvxRkQ+iixcX8yv5xVG5VxZbgvPrH9srHEqZWrdejDaGD6NU1UfEzXLHkBr6rrKePHT27xWvWda2rgCBysa8Z/DA5h98Gr2nDzLVDBDca05leKv1/WImeBKwpcQb6R2TRQZ2TOrWieoQASSwAvPyWRkzyzW/3EIfx+dE/HcQqFS64DXNNoYPse/fxhMqatelXXYGQV3o+ca9e3rVpSGXzoBQClO1kln3Pn5OK+vYELvp73rtw73v5kph7mh2WI+6nQbb3aYzJNXnFFNoENZJK/UOqYWtcSFCzUJEfkwMAsJ88Uj7FkZaeT1b2M59s2CYm+UyMieWWSkpUR7qpY4bFjgnsiczpvf9EbmbCtrg0srtIaxu+ZyYn599FxH1Vo54aAUlQ6nOwzTRdsRX5tG5vRO28CQrwbAa0743whv7ZxQomJiHUEjceFCTUJEPgx8Q8KgurWemuLk76NzWDnpIqaO7MZjV3fDaSKqvsk3C9cXc+xkRayn7sWlNQ8s2EjjdPs3ls+OdWPo9hmcvfFt2m18hye+HVtF0ze/l8OcVXk0PX4gwt60Dorqt+WW/i9VE/pTVELxIqOz1bILeejC1Go3U6sSzReeE9t8DIkLF2oSIvJhMrJnFisnXURWRlq18Dz/zMmRPbOotBA8zyP89CXbTBcIY0lZuQutCfhUEoinvhvL6J2PsfN4Syrdmj5211wOvNncG3OfWlEWtnV/0lmPcefnV0muMuW75Qz5ejAvDvmuSiz2GanmN7APt8Y2szqSuHCppyNEG1l4jRCrGGz/EL9gi3F2Y7mjjQLy+reJuJ0hGJaz1nB75r+YdNYr3jXZ/Owxp2rlRBiCWb/8CM+uuaN66KUJO4+3ZFLx3Xx2rFuV7WbhlzUBicoRwkUWXmOIXf9rsEf4RPlrW2ak8eHWA1G5wVRqIyLm2QOj+N3eX+HShqB7auVUKZ8QDkpxrO4ZjDs/n7TrSgO4cgzap+5jXvsH+FXz16ps93zWNc1qlmqNQiwQkY8Qu/7XYI/wE4d2MuK644hnnrGI+vBE56w80s2r6Z7yCXd++Qzoyogamhyvk2bLlaMU3Hvmazyb/Sda1/3G+56jUS4h2gSLyqlpNyUhORB3TRSIVnZjz0eWxi1ePstnnrEu39C67jc82OI5Lm64BqeqBKp6bSb0fppnO9xJpXJEVD5B6Uru2D6zWnKVzxAjtL9ecxYc7M8TxcPZc/KsKmMSGcseyKU3cWgnceUIlkgyVJLQdtK7tsYpIi/B7ivy8S7f0LruN0xpMYuLzlhbTexndZyAVhE8YLr/njuXfM7m93KCDnXh4P0f+zB1/8/Zc/KshPrrA/nkrUo8SIKVAOKTTwoC9Y9tnJ5Sxc2T179NxL1mi0vKmPivDSxcX+x1JZmFeUYbp1Ic0i25tWgKg7Y9z5KSflRoB1obSVb/XDXuVAhmOPj0plXXV+CwiLn3DK2jKhna6FM+6nQbO7peyStn/1/IfWujRSCXniRYCeESUWcoIXpMX7LN0jof1r0FU0d2q7bdTteoQJRXaqYs2uy9fqAyB5Hi61po535i2XPyLH6x+w/Aqbo5l5Yv4dtdzXk9ewz35EbYvUo50UBR/bbc3P9FAMuoHEPwNYPqrzJi7+s2gezRcO590KB9WO85HKw6lbXMSDO15CXBSgiGWPI1hEAWmW9mrAcz0Q+HkrLymPewzUhLqeI7NhOmPSfP4g/7JtD/i1c5e+M7zFp6MfNeupzK1yKPtwcod6Yy7vz8Uz1pg3Hye9g+E97tCsWLw75utKipCVayGFzzEZGvIQSyyKzC6EKpoRMIO774rIw0xvVvE9J5G6en8PfRORQ+NKSKdRqsLAQYou9x6VQWHGZ3fgvmrMyjiW82baiROUoZZROud1mWTqiGqwz+dwW80Qxec8C/s2DNhLi7dGpi442aGKEkVEcWXmsIwRY/reqnx2PB1Cq6w4px/duYPmn41ohXKjzDfHDDNTyb/X/UcxhRSA2uPcyxumeEfiIPWnPnl89YRuRY4qgHFyyArMvDv3aMiXVNe6m2WXOQhdckINjip5mlb2bd/X10DrumDWPXtGFBLX0FQYuieVwCUxZVb3xtRopTkZvdpNp2X6sPwhN4p1L870gfxu1/kR9Se6A1PLvmDuq6ToR+Mg9u695TAdM2lSfgf8OMJuXz6sOqcQlbsDUjHla2LAYnByLyNYiRPbP463U9QvK9emrofD1tGCsnXRSSWyQjPYUpw7tUG+NbRfOxqw2LvKTMXvx+uUubupZC6bNqRlqKk79e14Ovpw3jXxPH0fjqQm4r/gu5W1fywic3Vy2KFoYbx1MB85LB/wl9cq5S2JUP75xbI/z3EJ/sWam2mRxIdE0NwyPSZo/ZoT5+e/ZZNTkpKS0PeD0PA6Z9ENJ7MLPkwrHuPPkAWRbv9YPvO/P+9zNgO5y7dBG3N1vAFRkryHAe4bW2Y7ij7yyO1mloPzJHKd5vMQQ1xoVTV+JSDrJLd/No4e9t1cqh8qRh3TvToPXV0P2RuEbm+BIPK9sqQSvYYrC0RowvIvI1ELMwOn//u+fx2zM+0LmsEmk8FpdV2J6HUIXBzJKzCgG0wk73Jt9zeqJz/rBvAn2a/sDTDV7ix/mNuCs3xGxapQCFy52QVVS/Lbf3ew6wDr+shqvMsOx35Ruv01pCqxFxDceMR8ilHQPBn3D/joXwEXdNkhDJ47ed8LtAoXChCIOVJRdqqJ+d7k1W7qjtpc1Z3eZV1IivmPHjFlxvtWHOqnFhF0crrVOfcefn24vGMaNsnzscs0vc3DnxCrkM5C40Q4qwxR8R+SQhksfvYOF3wRbpgvn201IctsL6Qvljs3Nj8bwv/8Ynntj/hdvrQZ8ZcFUxeQPyOXryIBeXbAy7mUlR/baMOz8/PL89gOu44c6JwyJtNEIuYxEDL4u18UfcNUlCpI/fgVwygawr3+N+O3+DaVZsk/r1gobMTV+yjUpbM7W2OH19uY3SUlDKWFcwa2VYVu5iyqLNVd9zg/YsA/KBeypdHAqnIJrXb2+8m6YnDvJkwT32XTlguHH2LICBb8Q0BDOYGw6s/eOxcqtI5m78EUs+SYjl43cg68pjzd07r9Cy7IEdK8yupaaAa3pbr0l4njZKysr5obQcDZbzKikrN7U+84CDDicj1u5BHT0RekSOp9O4UhxKzQwtk9aDqww+vsaw6v+dlZBEq0BPcLFyq9TUzN3ajIh8khDLjEcrKyojPaWKCAQ7Php+fY15e75wQzCtRGnh+mK2LdpCm6nLyH5gMdkPLKbZvEIyjhnlE5yVFSGJfqAG5Ja4jhtWfdk+QPv47uNTSiGQkMfKrVITM3drO5LxKliWuK1XxxE0Pt5TeAwIWO88lOxcs+zecNsjWpUOtpOtmX/8W8bVax66S8f9nQrLleNLjEMxA7WutHKrSDZrzUQyXoWAWFlXhwMIvMKoTVOvjoN75xXy2/kbAj7em13Df8HUg1UIZjhkWFzDjqWal3omF4dTf8HPldPg2sPhReV4QjEXdYAdL4R+fBACJTNF260SyiJuooqe1dZia2LJC5aE2qnIjEBNOEJpXB1unZ6MtBQKHxpS5TxWeQNQ3VJduL6Y8ScqONyntdGpHMLrXqU1oENLrgpEvebQ5pqIYu+Dff7RSlqK9P85Hh2wkr2JeiBLXqJrBEsCZTTa9ZEHssBDSabxHVtcUlalO5ZDGU3EzfB9Ggl2ozCzVKcv2UZGSRkZCzcBcHB4F46dlx1efXtUeMlVZpz4DrbP5Pi2F1mfPZvzLrgx5FME+/ztROfYIVj0Vrhjo0mirhsPROQFSwKJwL0WpRJ8sfN4738NX/eO1Vh/oXYqRaXFE6nvTSbQjcmqdIK/W6fZos2k7v6BH67sQqWvKygE0S+tU5/JOf8XuTUPpDpO0Gf3zaz4OI0LBo4K+fhQhDxcyz6URdxExdFbnb+4pIwB0z6IaemFWJd5iEjklVLTgSuBk8BXwM1a6xL3vgeAWwEX8Cut9ZLIpiokglA7FXkE1+4fa6jx2GZCXV6pSUtxcLy8sspCov9NxuqLrMByMdHsfTYo3McZG/Z5nx6OXNWN7/u2Dknod6eHVps/EHVUJQOLroNvmkL2dWG7cAKJTSRx86HExicqjj5Q2Y1Yll6IR5mHSBde/wt01Vp3B74EHgBQSnUGrge6AJcBM5RSgbtECEmF1cKcp1KknRR3sJfm7rsgZvVFLCuvrLLI6t+NCsKrmmiV7evrHmqyaBPN5xdWjbkPQpvSPQDkZ4+h7YivA/aitYNSwMlDYYdgBst6jnVZjXDGRpNgWd2xKr0QjzIPEYm81nqp1rrC/XI10Mr9+wjgda31Ca3118AOoG8k1xJqFtGKdw72eO4vPlYo4IfSU/73ExXV82svPCfT9Fir7VD9fZrV+y93adLW7/PG3Nf/pCig0KcDj574nvx2N3F7v+coqt8WrRwU1W/LDefPCT3e3h9XGXx8bUhJVcHEJpCVG4xQ/lYSFUfve10rYuEyiod7Kpo++VuAee7fszBE38Ne97ZqKKVuB24HaNMmeo+wQuyJxsJcsMdzOwu8vouwHswWzcySrAJt9+D7Pj1NyAPh8duXDO2EKyMNdewkTofClV6XNsCjQF6TnrTt9xyljqohntqn+uW48/P5RZ9ZPLvmjtD9964y+N9wOPmDkWTlTAeUUfs+rUW1qpjBxMaplGlmsVWTG39C+VuJ1oJvqHiuaxVVFguXUTzcU0EteaXUMqXUJpOfET5jJgMVGGVBQkJrPVtrnau1zs3MtLaohNpJsMfzYBZNRlqKpYXvf6xdqykambsNCvfR6s8fkv3AYtpMXUarR/5LJbALo6wCwG4VxMZSimN1z2Dc+fnhWfeHN7uzaTHE3XUMq8zaYK4sq9IRVtuTmXi6jOJxraAir7W+RGvd1eTnLQCl1HjgCiBPnwq6LwZa+5ymlXubIFQh2ON5MFGtX6+O5SO2/7F2fPLBfNMXnpOJv+2a4lSkOAJbtGbXrvvj8YDHeFEKfNw5IdfJscLHrRNMbKw+42g1k69JxNNlFI9rRZQMpZS6DPgb8FOt9QGf7V2A1zD88C2B94EOWuuAz92SDCX4Y6fB+ROjc2wlsthJeAk1AUwBef3bkJvdhCmLNpuWgUhxKKaPqt4AJfP19Ry6uhu6boheU61RuhKtVHSSq5zp4CqjrE5z3vuhH38vHo4rvV3A6BqwlywkXaDiQyyTof4B1AP+qwzf3Gqt9R1a681KqfnAFgw3zl3BBF4QzAhW5rhlRprtpCo74wK5dMzWBzwF1aaOPJUl+vDbm72LwBlpRh9dM2HruOsHti3Y6PXdA7a7V2l3sFpUkqtcpQCkVXzL1Q0XcfU5iwAF+zpDy3/AmYOlC1QSI2UNhKQgXmnngSz5fQEifDz7Q7FW/d9T2Nm0AFqTXVoUnZIJ/nSdAt0fCvkwO0XghOggBcqEpCdeftJAvmkrn74CSx9+IPzfU4+PdvLroh9o6hkQYo17j1UfduilFZumwGsK5qbA6+nG7zZq30sXqJqBWPKC4IfdbklgHr4J9hqRB2PAtA/YMKh9yNZ99rFd7HqrXdjXDQlHPbhggWmHK7Hk44cUKBOEELCK0zbzS1slA7m0jtj/vK+krHqtHBti7ymZkJ89hsk5/8fu9Da0iVb1S38qT6CXD+OHysaUNL2S9oMe8sbeBypwJ8QPseSFWkGiojisrFUPkVit/uc+mtPSu0jrVAqrSIbsilIeXTOB2/s8Q2md+t7tShu1faJW7tiESpw4+j0LZ98KSHRNvAhkyYvIC0lPImuB2w3xDEfogr2vfIxU8VKfY9KB2cDkynKKHOYNUwDSK44x+9Ofx0ToAej7PPzk1pAOkRtC+IjIC7WaRPt+F64vtgzxbJyewvHyyrBvQMGELx+YDOyGUyUTMCIqgn2zY+q7V0648kvb1TCTvWlHohGfvFCrSXQUh1Wd+7QUJ1oTUTOKYHVc8jhVJsGXNkBRkHPvTm8TO7+9dsHnf4SUM6BovlEh0xdVB3QFpLWEViPIX34+ZeWNqwypLU07Eo2EUApJTzglhKNNqH1yY30DehTDdROIlLKT3N7/pSpVMI06OZXUub488vIJu/KNGjn+Ag+GwIO3js6rLW5jcMM11YZJuGXkiMgLSU+iapD7M7JnFisnXVSlnn6ibkB5GL75bDCNt1cnKyivVJQ661U90N2E3OWow8yOd9Hg2sMR17q3Q5rjBLOyH6N13W+qbI/njbq2IiIvJD2JqkFuh0TegPIwql6e/+cPaTqvEOcPpaA1zh9KabJgI7p+3cAncFfB1LEojmZCquMkf856kj+1nMHqc29kZ7crWdZ+bNCkKyEwsvAqCEGINOoj0VEjVouah/5wCQdCLI6mdCX/XDUuZlE5WlukAgRIuhIkukYQwubBhRvJX727Wu/YmvKkYBezG82xnlnVQjBtoTVO7eL27bOYUXB3DGZrgaMuXPFFWP1razsSXSMIYbBwfXE1gYfkjPoIFKUzmeCROFVQCpcyfPaAidBbFXuIkMqTRsTO+XOif+5ajPjkBcGC6Uu22e46lax4/PYauBPwrB44IHhxNKWY2fEu1JhK1JhKml1zkPx+L0DbsbGb8J5/x+7ctRQReUGwIJCQ18aojxkYzR804ALOLNhrS+g9P4fqNeXms28hv9dfwRmjz8dd+56jO9m5+CYOvNqMynwHB15txs7FN8kCrQki8oJgQSAhLz1ZYdoDtjYxy+mgxRufe6Ny7FAOTE49Ewa+ETuhL15MxdtdaF/yKpl1DuFQmsw6h2hf8ioVb3fx9q0VDETkBcECs/BHAIeCH0rLQ64fn2yM7JnFjJ80o9+zq2nwSZFtod8NRhTMsE3k93uBtiN2Ry/W3pEGH19LHW3eH7eOPu7tWysYSHSNIATAPyrl2IkK0z6up0ON9AkYCVbB+nhmY/j5zQqooTVNTxziyYJfhReG2bADHNkefFyHO6HPjNDPn6RICKUgRIl2k941XYxVwNfThsV7OgkjH7gFOOm3PQV4CWNBty3WUTsprnKeWnkXt+95HoW21xPFUQ/qZsDxb4OPTWsJV9W+pysrpP2fIESJmlAnpyaQB7wIp1oVun/3CDy43TYWlDtTuHPgszjHujjzyv3ktwnsxjlRWddIhjr+nb0JHv8m+JjTBBF5QQiBmlInpyaQBxzEiMbR7t99K2K2CXYChxGVc+CMsxjf92Web32zcS596ueH8vrM/WEY/+uw3PDzp7WwN7nUs0J9O7UWEXlBCIGaXCcnUhauL2bAtA+iFjVkpxKmh4q6dbm9z2xSxmocYytxjHFxRruvuWL/ItLOf5Yh/c4zBrYaYe+EdsedBohPXhCEmDXtyAduBCrtDDYpXFMfeBafJ4SjO+HdruAKkIzmTINhm6FBCA1Rju6ELx6HvW8Z5Y+d6YAy4vLTWhg3jXPvq7ElFcQnLwhJQLQt6VCYvmSbZXOTSMgDXgWC1Ls0MFl9PYaxwJvv2dCgfeAYfGeasT8UgS9ebNw4ts80BB4McXcdA7S35j3vdk3KGHwReUGoAXgs6eKSsoTE38eyu5ZnkTY70KAAHoWTwD2+G9wx+HS404iiUQ7j3w53GhZ8KJUqj+404uoDPRl4cJUlZQy+iLwg1ABiZUnbJdZRQ54aOXOo7qe3Ez1ZrbdUg/ZGHPxVxTDGZfzbZ0ZoFjwYLho7Au/BVWYck0SIyAtCDSDRfWrjFTXk27FKuf/9J8CJiqhexzZ734rPMQlERF4QagCJjr+PZ9SQx6qvdP+bB3RY8iW4rJdnm5psy8dIuHK4/803GROUsv2hH5NkMfhST14QagATh3YyjW6JZ/x9oJrzseYvbTKYsGAj315+LpXpKVUWYVOAJ/3G+5dMKHK/hqqx+kFJa3FqsdUuSRaDL5a8INQAanP8vR08xdD6P/0xzeYVUu9wGUprsqmaRethMtU7WpW6t4dEOPH0SRaDH1GcvFLqT8AIjCev74DxWut9SimFcfO9HOOzH6+1XhfsfBInLwiCHRyY955S2IzJ92An7t6XcGLw40As4+Sna627a61zgHeAP7q3/wzo4P65HZgZ4XUEQRC8WJVMCFpKwZ9gcfe+hBODXwOISOS11j/6vKzPqZvrCOBVbbAayFBK2Sw6IQiCEBizkgnp7u0h4x93jzIyXp31gQhi8O1wdCesmQD/zoLXHMa/ayZENRY/4rIGSqlHMTKXDwMXaq0PKKXeAaZprT92j3kfuF9rXc0Xo5S6HfeaSZs2bXoXFYXUUlgQhNOUfAwf/G4MC/5RQlx0TTTFi60TsTxPDTZvKhG5a5RSy5RSm0x+RgBorSdrrVtjfOa/tDUjH7TWs7XWuVrr3MzMzFAPFwThNMUsFDNpCJZpG8Xs2qAir7W+RGvd1eTHPyMgH7jG/Xsx0NpnXyv3NkEQhBpJVOLu7WIn0zZK2bUR+eSVUh18Xo4Atrp/XwTcqAz6A4e11mFkHQiCIMQeT9x9EcbCoifuPmZCbzdrNgrZtZFG10xzu24+B4Zwqo7QYmAnsAN4DqM9pCAIQo3EKu5+HDGy6u1m2kYhuzaijFet9TUW2zVwVyTnFgRBiBeBWhWGnU0bCLuZtlHIrpWMV0EQTnuCxdeHlU0biDh2uBKRFwThtMdOq8JA1n7InHtf8AQsZxqcOzHiS4nIC4Jw2uNbAtmKkLNpAxGLDlcWiMgLgiAQuLFJ2Nm0gYhmh6sASKlhQRAEHzyLq3HJpvV0uOozIxZnB0TkBUEQqpFHkmXQBkDcNYIgCLUYEXlBEIRajIi8IAhCLUZEXhAEoRYjIi8IgpBAYl39UqJrBEEQEoSn+qWnOFos6uSIJS8IgpAgrKpfRrNOjoi8IAhCgrCqhxPNOjki8oIgCAnCqh5ONOvkiMgLgiAkCLPql9GukyMiLwiCkCB8q18q97+ziW5JBYmuEQRBSCCxrpMjlrwgCEItRkReEAShFiMiLwiCUIsRkRcEQajFiMgLgiDUYpTWOtFz8KKUOgAcAw4mei42aIbMM5rIPKOLzDO61PR5ZmutM8121CiRB1BKrdVa5yZ6HsGQeUYXmWd0kXlGl2SZpxnirhEEQajFiMgLgiDUYmqiyM9O9ARsIvOMLjLP6CLzjC7JMs9q1DifvCAIghA9aqIlLwiCIEQJEXlBEIRaTMJEXil1mVJqm1Jqh1Jqksn+ekqpee79nyql2iZgmnbmOV4pdUApVej+uS0Bc3xRKfWdUmqTxX6llHrK/R4+V0r1ivcc3fMINs/BSqnDPp/lH+M9R/c8WiulPlRKbVFKbVZK3WMyJuGfqc15JvwzVUqlKqU+U0ptcM/zYZMxCf++25xnwr/vIaO1jvsP4AS+AtoDdYENQGe/MROAWe7frwfm1dB5jgf+kYjP0WcOg4BewCaL/ZcD72GUrO4PfFpD5zkYeCeRn6V7Hi2AXu7fGwJfmvy/J/wztTnPhH+m7s+ogfv3FOBToL/fmJrwfbczz4R/30P9SZQl3xfYobXeqbU+CbwOjPAbMwJ4xf37G8DFSikVxzmCvXkmHK31R8D3AYaMAF7VBquBDKVUi/jM7hQ25lkj0Frv11qvc/9+BPgCyPIblvDP1OY8E477Mzrqfpni/vGP+Ej4993mPJOORIl8FrDH5/Veqv9xesdorSuAw0DTuMzOZA5uzOYJcI37kf0NpVTr+EwtJOy+j5rAee7H5feUUl0SPRm326AnhlXnS436TAPME2rAZ6qUciqlCoHvgP9qrS0/zwR+3+3ME2r+970KsvAaOW8DbbXW3YH/csoaEUJnHUYNjh7A08DCRE5GKdUAeBP4tdb6x0TOJRBB5lkjPlOttUtrnQO0AvoqpbomYh7BsDHPpPu+J0rkiwHfO2Ar9zbTMUqpOkAj4FBcZmcyBzfV5qm1PqS1PuF++TzQO05zCwU7n3fC0Vr/6Hlc1lovBlKUUs0SMRelVAqGcOZrrReYDKkRn2mwedakz9Q9hxLgQ+Ayv1014fvuxWqeSfJ9r0KiRH4N0EEp1U4pVRdjoWWR35hFwE3u368FPtDulY84EnSefn7Y4Rh+0ZrGIuBGd0RIf+Cw1np/oiflj1LqLI8fVinVF+PvM+5fdPccXgC+0Fr/zWJYwj9TO/OsCZ+pUipTKZXh/j0NuBTY6jcs4d93O/NMku97FRLSyFtrXaGU+iWwBCOC5UWt9Wal1CPAWq31Iow/3n8qpXZgLNZdX0Pn+Sul1HCgwj3P8fGep1JqLkYURTOl1F7gIYxFI7TWs4DFGNEgO4BS4OZ4z9HmPK8F7lRKVQBlwPUJuLEDDABuADa6/bMAvwfa+My1JnymduZZEz7TFsArSiknxk1mvtb6nZr2fbc5z4R/30NFyhoIgiDUYmThVRAEoRYjIi8IglCLEZEXBEGoxYjIC4Ig1GJE5AVBEGoxIvKCIAi1GBF5QRCEWsz/A5Bih9KLcPu1AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(x_n[:, -1], y_n[:, -1])\n",
    "plt.scatter(\n",
    "    x_n[:, -1],\n",
    "    x_n.dot(w_grad)[:, -1],\n",
    "    color=\"orange\",\n",
    "    label=\"Handwritten linear regression\",\n",
    "    linewidth=5,\n",
    ")\n",
    "plt.scatter(x_n[:, -1], lr.predict(x_n), color=\"cyan\", label=\"sklearn Ridge\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если в полученных решениях есть небольшие различия, это не страшно. Модель основанная на вашей реализации не использует свободный член (он равен $0$), в то время версия из `sklearn` настраивает и его."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6GgeWdBmGE3H"
   },
   "source": [
    "### Сдача решения\n",
    "Сдайте в чекер реализованный класс `LossAndDerivatives`. Для этого можете скопировать всю ячейку с кодом (в том числе и импортирование `numpy`) в файл `derivatives.py`.\n",
    "\n",
    "На этом задача завершена. Поздравляем!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "assignment0_02_linear_regression_and_gradient_descent.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
